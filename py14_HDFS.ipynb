{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e512b8-6430-4588-8964-881ae833ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pkdata/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef141ed-ebac-4c62-b0f5-a7d461afe6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.18.0.2 172.20.0.2 \n"
     ]
    }
   ],
   "source": [
    "!hostname -I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50b4d0f-78d7-4039-8112-b628501f5c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.19.0.2 172.18.0.2 \n"
     ]
    }
   ],
   "source": [
    "!hostname -I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a9df4d-5eee-44b5-b309-596d7e5b3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc5a879-3690-4fd3-bf27-0be091a7d80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7863864-9e7b-42c9-a178-3ab0f5aa57f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TION, AND DISTRIBUTION\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "      \"License\" shall mean the terms and conditions for \n"
     ]
    }
   ],
   "source": [
    "user = \"hadoop\"\n",
    "host = \"http://namenode:9870\"\n",
    "path = \"/user/hadoop/LICENSE.txt\"\n",
    "hdfs = InsecureClient(host, user = user)\n",
    "with hdfs.read(path, encoding = \"utf-8\") as reader:\n",
    "    text = reader.read()\n",
    "print(text[200:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4980992f-ab90-4562-9a25-c185fe5f68ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache   1\n",
      "License   1\n",
      "Version   1\n",
      "2.0,   1\n",
      "January   1\n",
      "2004   1\n",
      "http://www.apache.org/licenses/   1\n",
      "TERMS   1\n",
      "AND   2\n",
      "CONDITIONS   1\n",
      "FOR   1\n",
      "USE,   1\n",
      "REPRODUCTION,   1\n",
      "DISTRIBUTION   1\n",
      "1.   1\n",
      "Definitions.   1\n",
      "\"License\"   1\n",
      "shall   1\n",
      "mean   1\n",
      "the   1\n",
      "terms   1\n",
      "and   1\n",
      "conditions   1\n",
      "for   1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = text[:300].strip().split()\n",
    "words_counts = Counter(words)\n",
    "# print(words_counts)\n",
    "\n",
    "for word, count in words_counts.items():\n",
    "    print(word, \" \" , count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fdf8101-2439-4a84-a304-5326a4fe40e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이염.txt\n",
      "LICENSE.txt\n",
      "QuasiMonteCarlo_1744770668017_2134536809\n",
      "QuasiMonteCarlo_1744770998880_1333029022\n",
      "starbucks.csv\n"
     ]
    }
   ],
   "source": [
    "hdfs_dir = \"/user/hadoop\"\n",
    "show = hdfs.list(hdfs_dir)\n",
    "for s in show:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f95fa5f5-aca2-4778-b3aa-ac8d79532fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "local_path = \"/home/pkdata/data/dataset/starbucks_20250411113927.csv\"\n",
    "hdfs_path = \"/user/hadoop/starbucks2.csv\"\n",
    "hdfs.upload(hdfs_path, local_path, overwrite = True)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd9c8c1c-fd20-4630-940c-adb0a0464518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accessTime': 1744780106717, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 16456, 'group': 'supergroup', 'length': 17957, 'modificationTime': 1744780108537, 'owner': 'hadoop', 'pathSuffix': '', 'permission': '644', 'replication': 3, 'storagePolicy': 0, 'type': 'FILE'}\n"
     ]
    }
   ],
   "source": [
    "hdfs_path = \"/user/hadoop/starbucks2.csv\"\n",
    "if hdfs.status(hdfs_path, strict=False) == None:\n",
    "    print(\"파일 없음\")\n",
    "else:\n",
    "    print(hdfs.status(hdfs_path, strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af1ba7c3-eebf-486a-a09f-999255652efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제 완료\n"
     ]
    }
   ],
   "source": [
    "hdfs_path = \"/user/hadoop/\"\n",
    "del_file = \"starbucks2.csv\"\n",
    "if hdfs.delete(hdfs_path + del_file):\n",
    "    print(\"삭제 완료\")\n",
    "elif hdfs.status(hdfs_path + del_file, strict=False) == None:\n",
    "    print(\"파일 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10418a8b-a8b4-4a24-993a-e9de73fe29d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
